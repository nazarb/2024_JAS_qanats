{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b279cba-ef23-4744-975d-3eb91ebd158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae806ab-008f-4f35-8e41-dd8a21169682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ML\\yolov9\n"
     ]
    }
   ],
   "source": [
    "cd C:\\ML\\yolov9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e7cf44-796f-4912-95f1-c47c9e3710a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d3a4fb-05ec-4af2-b9c1-99d098d9d034",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "Setup complete âœ… (20 CPUs, 63.7 GB RAM, 421.1/475.8 GB disk)\n",
      "Setup complete. Using torch 2.0.1 (NVIDIA GeForce RTX 3060)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f565b87a-e67a-48bc-a523-c13be9082550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ML\\yolov9\n"
     ]
    }
   ],
   "source": [
    "cd C:\\ML\\yolov9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6502d-125a-47ae-80f6-103fd08b60a3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5c6ca-9a31-45f8-b5ba-636f2ead82f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python train.py --device 0 --workers 8 --batch 32 --img 256 --epochs 200 --data c:/ml/yolov9/Datasets/qanats_256_synt_G1_AFG1_pairs_single_2.yaml --weights c:/ml/yolov9/Datasets/gelan-e.pt --cfg c:/ml/yolov9/models/detect/gelan-e_qanat.yaml --hyp c:/ml/yolov9/data/hyps/hyp.scratch-high.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b95ae-3b9f-4214-85a6-6b766f253ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63c494c-915b-4002-96ae-f2efbb98822c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=c:/ml/yolov9/Datasets/qanats_256_synt_G1_AFG1_pairs_single_2.yaml, weights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], batch_size=32, imgsz=256, conf_thres=0.001, iou_thres=0.7, max_det=300, task=val, device=0, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs\\val, name=gelan_e_256_val, exist_ok=False, half=False, dnn=False, min_items=0\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "gelan-e_qanat summary: 690 layers, 57285782 parameters, 0 gradients, 188.6 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\ML\\datasets\\qanats_256_synt_G1_AFG1_pairs_single_2\\labels\\val.cache... 589 images, 172 backgrounds, 0 corrupt: 100%|##########| 589/589 00:00\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\ML\\datasets\\qanats_256_synt_G1_AFG1_pairs_single_2\\labels\\val.cache... 589 images, 172 backgrounds, 0 corrupt: 100%|##########| 589/589 00:00\n",
      "\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/19 00:00\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   5%|5         | 1/19 00:01\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  11%|#         | 2/19 00:02\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  16%|#5        | 3/19 00:03\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  21%|##1       | 4/19 00:04\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  26%|##6       | 5/19 00:04\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  32%|###1      | 6/19 00:05\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  37%|###6      | 7/19 00:06\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  42%|####2     | 8/19 00:06\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  47%|####7     | 9/19 00:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  53%|#####2    | 10/19 00:08\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  58%|#####7    | 11/19 00:08\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  63%|######3   | 12/19 00:08\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  68%|######8   | 13/19 00:09\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  74%|#######3  | 14/19 00:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  79%|#######8  | 15/19 00:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  84%|########4 | 16/19 00:11\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  89%|########9 | 17/19 00:11\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  95%|#########4| 18/19 00:11\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|##########| 19/19 00:12\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|##########| 19/19 00:12\n",
      "                   all        589       9080      0.912       0.89      0.941      0.541\n",
      "                 qanat        589       4931      0.924      0.883      0.932      0.503\n",
      "            qanat_pair        589       4149        0.9      0.898      0.949       0.58\n",
      "Speed: 0.1ms pre-process, 10.2ms inference, 1.6ms NMS per image at shape (32, 3, 256, 256)\n",
      "Results saved to \u001b[1mruns\\val\\gelan_e_256_val2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --data c:/ml/yolov9/Datasets/qanats_256_synt_G1_AFG1_pairs_single_2.yaml --img 256 --batch 32 --conf 0.001 --iou 0.7 --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --name gelan_e_256_val  --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d51f683-2a38-4fca-92f5-5d84b2f8e43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir runs\\train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661cd59-a4c1-4d73-8344-0bd03e779b37",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2290fad2-e46c-45ab-824f-1e27d6834e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.200  Python-3.11.3 torch-2.0.1 CPU (12th Gen Intel Core(TM) i7-12700)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'c:\\ML\\ultralytics\\runs\\detect\\qanats_256_synt_pairs_single_3c_\\weights\\best.pt' with input shape (1, 3, 256, 256) BCHW and output shape(s) (1, 6, 1344) (130.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  4.9s, saved as 'c:\\ML\\ultralytics\\runs\\detect\\qanats_256_synt_pairs_single_3c_\\weights\\best.onnx' (260.0 MB)\n",
      "\n",
      "Export complete (9.1s)\n",
      "Results saved to \u001b[1mC:\\ML\\ultralytics\\runs\\detect\\qanats_256_synt_pairs_single_3c_\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=c:\\ML\\ultralytics\\runs\\detect\\qanats_256_synt_pairs_single_3c_\\weights\\best.onnx imgsz=256  \n",
      "Validate:        yolo val task=detect model=c:\\ML\\ultralytics\\runs\\detect\\qanats_256_synt_pairs_single_3c_\\weights\\best.onnx imgsz=256 data=qanats_256_synt_G1_AFG1_pairs_single_2.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\ML\\\\ultralytics\\\\runs\\\\detect\\\\qanats_256_synt_pairs_single_3c_\\\\weights\\\\best.onnx'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the model\n",
    "model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6306437-d8db-4645-91f6-2a8f4c9c3129",
   "metadata": {},
   "source": [
    "# Detect"
   ]
  },
  {
   "cell_type": "raw",
   "id": "351af379-213e-41a3-b80b-33998a5406f7",
   "metadata": {},
   "source": [
    "usage: detect.py [-h] [--weights WEIGHTS [WEIGHTS ...]] [--source SOURCE]\n",
    "                 [--data DATA] [--imgsz IMGSZ [IMGSZ ...]]\n",
    "                 [--conf-thres CONF_THRES] [--iou-thres IOU_THRES]\n",
    "                 [--max-det MAX_DET] [--device DEVICE] [--view-img]\n",
    "                 [--save-txt] [--save-conf] [--save-crop] [--nosave]\n",
    "                 [--classes CLASSES [CLASSES ...]] [--agnostic-nms]\n",
    "                 [--augment] [--visualize] [--update] [--project PROJECT]\n",
    "                 [--name NAME] [--exist-ok] [--line-thickness LINE_THICKNESS]\n",
    "                 [--hide-labels] [--hide-conf] [--half] [--dnn]\n",
    "                 [--vid-stride VID_STRIDE]\n",
    "detect.py: error: argument --imgsz/--img/--img-size: invalid int value: 'show_labels=False'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d26c1-7a28-422c-8caf-4fb28e2d2c2d",
   "metadata": {},
   "source": [
    "## Morroco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9bd28-6ba1-4f84-9525-57754666fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs\\detect\\exp ## D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_ab.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d12dbd-bcc9-4aad-82ba-8343eb1a8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db255149-4798-4818-9aa9-ad880af7ad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_ab.tif, data=data\\coco128.yaml, imgsz=[4096, 4096], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "gelan-e_qanat summary: 690 layers, 57285782 parameters, 0 gradients, 188.6 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\MR1\\D3C1211-100139A008_ij_clip7_b1_ab.tif: 4096x4096 629 qanats, 587 qanat_pairs, 4289.0ms\n",
      "Speed: 11.6ms pre-process, 4289.0ms inference, 80.0ms NMS per image at shape (1, 3, 4096, 4096)\n",
      "Results saved to \u001b[1mruns\\detect\\exp\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --device 0 --img 4096 --conf 0.1 --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --source D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_ab.tif  --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb9149-7d28-49e3-a4bd-af5d1bea4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs\\detect\\exp2 - D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_ab.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87d157d-2a14-4f92-b2d0-4ffdf8d42287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_ab.tif, data=data\\coco128.yaml, imgsz=[2048, 2048], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "gelan-e_qanat summary: 690 layers, 57285782 parameters, 0 gradients, 188.6 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\MR1\\D3C1211-100139A008_ij_clip7_b1_ab.tif: 2048x2048 359 qanats, 340 qanat_pairs, 364.9ms\n",
      "Speed: 2.5ms pre-process, 364.9ms inference, 92.6ms NMS per image at shape (1, 3, 2048, 2048)\n",
      "Results saved to \u001b[1mruns\\detect\\exp2\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp2\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --device 0 --img 2048 --conf 0.1 --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --source D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_ab.tif  --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4ab3d-4f30-4046-91ac-839a05372a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs\\detect\\exp15 - D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_a.tif - runs\\detect\\exp15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f3a77e-214e-4b29-b5c8-1650dd2d3621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_a.tif, data=data\\coco128.yaml, imgsz=[4096, 4096], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "gelan-e_qanat summary: 690 layers, 57285782 parameters, 0 gradients, 188.6 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\MR1\\D3C1211-100139A008_ij_clip7_b1_a.tif: 4096x4096 2246 qanats, 2194 qanat_pairs, 4460.3ms\n",
      "Speed: 13.6ms pre-process, 4460.3ms inference, 160.7ms NMS per image at shape (1, 3, 4096, 4096)\n",
      "Results saved to \u001b[1mruns\\detect\\exp3\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp3\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_a.tif --img 4096 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d39d90-7f77-4049-8f7e-8efc610a9629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_a.tif, data=data\\coco128.yaml, imgsz=[7168, 7168], conf_thres=0.1, iou_thres=0.45, max_det=9999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "gelan-e_qanat summary: 690 layers, 57285782 parameters, 0 gradients, 188.6 GFLOPs\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ML\\yolov9\\detect.py\", line 231, in <module>\n",
      "    main(opt)\n",
      "  File \"C:\\ML\\yolov9\\detect.py\", line 226, in main\n",
      "    run(**vars(opt))\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\detect.py\", line 85, in run\n",
      "    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 936, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 854, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "                                                                                          ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\yolo.py\", line 582, in forward\n",
      "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\yolo.py\", line 481, in _forward_once\n",
      "    x = m(x)  # run\n",
      "        ^^^^\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 649, in forward\n",
      "    out = torch.sum(torch.stack(res + xs[-1:]), dim=0)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.38 GiB (GPU 0; 12.00 GiB total capacity; 35.82 GiB already allocated; 0 bytes free; 37.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1_a.tif --img 7168 --save-txt --save-conf --max-det 9999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd3b87-7cbe-4e88-80cc-606c1359bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs\\detect\\exp16 - D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ede0b8e-ebbb-46f9-922f-ac8bfbc818db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1.tif, data=data\\coco128.yaml, imgsz=[9216, 9216], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "gelan-e_qanat summary: 690 layers, 57285782 parameters, 0 gradients, 188.6 GFLOPs\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ML\\yolov9\\detect.py\", line 231, in <module>\n",
      "    main(opt)\n",
      "  File \"C:\\ML\\yolov9\\detect.py\", line 226, in main\n",
      "    run(**vars(opt))\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\detect.py\", line 85, in run\n",
      "    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 936, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 854, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "                                                                                          ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\yolo.py\", line 582, in forward\n",
      "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\yolo.py\", line 481, in _forward_once\n",
      "    x = m(x)  # run\n",
      "        ^^^^\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 648, in forward\n",
      "    res = [F.interpolate(x[self.idx[i]], size=target_size, mode='nearest') for i, x in enumerate(xs[:-1])]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 648, in <listcomp>\n",
      "    res = [F.interpolate(x[self.idx[i]], size=target_size, mode='nearest') for i, x in enumerate(xs[:-1])]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3931, in interpolate\n",
      "    return torch._C._nn.upsample_nearest2d(input, output_size, scale_factors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.06 GiB (GPU 0; 12.00 GiB total capacity; 38.65 GiB already allocated; 0 bytes free; 40.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:/UnderTheSand/Rasters/ML/MR1/D3C1211-100139A008_ij_clip7_b1.tif --img 9216 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa381e-d990-4e78-865d-9996b2ca8b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b7372-9737-40c6-a6ee-1a1f9cdb4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8_ad.tif - runs\\detect\\exp12\\labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d7e41d3-3469-495b-94d7-04ac2e433479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8_ad.tif, data=data\\coco128.yaml, imgsz=[3072, 3072], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "gelan-e_qanat summary: 690 layers, 57285782 parameters, 0 gradients, 188.6 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8_ad.tif: 3072x3072 1960 qanats, 1883 qanat_pairs, 852.3ms\n",
      "Speed: 5.5ms pre-process, 852.3ms inference, 213.4ms NMS per image at shape (1, 3, 3072, 3072)\n",
      "Results saved to \u001b[1mruns\\detect\\exp15\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp15\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8_ad.tif --img 3072 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57066ed-a24b-4a2b-a666-987ec9d21380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8_a.tif - runs\\detect\\exp13\\labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc046bb9-73e3-4df2-a89a-0db16e655c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8_a.tif, data=data\\coco128.yaml, imgsz=[4096, 4096], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "gelan-e_qanat summary: 690 layers, 57285782 parameters, 0 gradients, 188.6 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8_a.tif: 4096x4096 3122 qanats, 3024 qanat_pairs, 3043.2ms\n",
      "Speed: 13.0ms pre-process, 3043.2ms inference, 110.3ms NMS per image at shape (1, 3, 4096, 4096)\n",
      "Results saved to \u001b[1mruns\\detect\\exp16\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp16\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8_a.tif --img 4096 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d69bf-f182-4ace-b361-ecc63e1facf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97e4d0a6-8971-4fce-a300-9855116c76d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8.tif, data=data\\coco128.yaml, imgsz=[9216, 9216], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "gelan-e_qanat summary: 690 layers, 57285782 parameters, 0 gradients, 188.6 GFLOPs\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ML\\yolov9\\detect.py\", line 231, in <module>\n",
      "    main(opt)\n",
      "  File \"C:\\ML\\yolov9\\detect.py\", line 226, in main\n",
      "    run(**vars(opt))\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\detect.py\", line 85, in run\n",
      "    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 936, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 854, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "                                                                                          ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\yolo.py\", line 582, in forward\n",
      "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\yolo.py\", line 481, in _forward_once\n",
      "    x = m(x)  # run\n",
      "        ^^^^\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 648, in forward\n",
      "    res = [F.interpolate(x[self.idx[i]], size=target_size, mode='nearest') for i, x in enumerate(xs[:-1])]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ML\\yolov9\\models\\common.py\", line 648, in <listcomp>\n",
      "    res = [F.interpolate(x[self.idx[i]], size=target_size, mode='nearest') for i, x in enumerate(xs[:-1])]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\giap\\.conda\\envs\\ultrapgu9\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3931, in interpolate\n",
      "    return torch._C._nn.upsample_nearest2d(input, output_size, scale_factors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.06 GiB (GPU 0; 12.00 GiB total capacity; 38.65 GiB already allocated; 0 bytes free; 40.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:\\UnderTheSand\\Rasters\\ML\\MR2b\\D3C1218-401390F004_bc_8\\D3C1218-401390F004_bc_8.tif --img 9216 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d2d62-4bd4-4d5a-8093-1ef7d97f0844",
   "metadata": {},
   "source": [
    "## Gorgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b389fb-7810-4b8e-8126-47f47cc480c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs\\detect\\exp5-6 #D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1216-401091A016_a\\A016_a3.tif - is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c5a77d6-7a13-4eb0-966f-83ba88b5a37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1216-401091A016_a\\A016_a3.tif, data=data\\coco128.yaml, imgsz=[5632, 5632], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "gelan-e_qanat summary: 690 layers, 57285782 parameters, 0 gradients, 188.6 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1216-401091A016_a\\A016_a3.tif: 3552x5632 1556 qanats, 1784 qanat_pairs, 19301.7ms\n",
      "Speed: 13.5ms pre-process, 19301.7ms inference, 91.3ms NMS per image at shape (1, 3, 5632, 5632)\n",
      "Results saved to \u001b[1mruns\\detect\\exp18\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp18\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1216-401091A016_a\\A016_a3.tif --img 5632 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cff764f1-ddd3-46d0-af5e-95aff843b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1216-401091A016_a\\A016_a3.tif, data=data\\coco128.yaml, imgsz=[9216, 9216], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "yolov9_qanat summary: 580 layers, 60498220 parameters, 0 gradients, 263.9 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1216-401091A016_a\\A016_a3.tif: 5792x9216 2748 qanats, 2435 qanat_pairs, 54370.5ms\n",
      "Speed: 40.6ms pre-process, 54370.5ms inference, 73.3ms NMS per image at shape (1, 3, 9216, 9216)\n",
      "Results saved to \u001b[1mruns\\detect\\exp6\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp6\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1216-401091A016_a\\A016_a3.tif --img 9216 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f245fff7-047c-481c-8354-0c17102a007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1219-10139\\D3C1219_a3.tif - runs\\detect\\exp9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "522219e2-d24c-4cf1-999b-579142c79b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1219-10139\\D3C1219_a3.tif, data=data\\coco128.yaml, imgsz=[9216, 9216], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "yolov9_qanat summary: 580 layers, 60498220 parameters, 0 gradients, 263.9 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1219-10139\\D3C1219_a3.tif: 7264x9216 1382 qanats, 1268 qanat_pairs, 64618.1ms\n",
      "Speed: 43.5ms pre-process, 64618.1ms inference, 133.9ms NMS per image at shape (1, 3, 9216, 9216)\n",
      "Results saved to \u001b[1mruns\\detect\\exp9\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp9\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:\\UnderTheSand\\Rasters\\ML\\G1\\D3C1219-10139\\D3C1219_a3.tif --img 9216 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc5ec8-2b65-4a56-8e5a-27a36772f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs\\detect\\exp4#D:\\UnderTheSand\\Rasters\\ML\\G2\\ds1110-1089df038_wgs_1_a.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a8aa5b-4fb7-4aab-a74f-ac7804088dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:\\UnderTheSand\\Rasters\\ML\\G2\\ds1110-1089df038_wgs_1_a.tif, data=data\\coco128.yaml, imgsz=[2048, 2048], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "yolov9_qanat summary: 580 layers, 60498220 parameters, 0 gradients, 263.9 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\G2\\ds1110-1089df038_wgs_1_a.tif: 1632x2048 550 qanats, 512 qanat_pairs, 317.3ms\n",
      "Speed: 0.0ms pre-process, 317.3ms inference, 140.1ms NMS per image at shape (1, 3, 2048, 2048)\n",
      "Results saved to \u001b[1mruns\\detect\\exp4\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp4\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:\\UnderTheSand\\Rasters\\ML\\G2\\ds1110-1089df038_wgs_1_a.tif --img 2048 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa90322-a374-42e6-8c1c-5689c2f43437",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Afghanistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe121bb6-67f5-4fa6-b3b0-ab152494ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## runs\\detect\\exp10 -  D:\\UnderTheSand\\Rasters\\ML\\AFG1\\D3C1219-300971A040_c_10.tif: 5120x5120 (yolov 8) - try 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32374757-6eb7-4abd-9bee-2fcce3141099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:\\UnderTheSand\\Rasters\\ML\\AFG1\\D3C1219-300971A040_c_10.tif, data=data\\coco128.yaml, imgsz=[8192, 8192], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "yolov9_qanat summary: 580 layers, 60498220 parameters, 0 gradients, 263.9 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\AFG1\\D3C1219-300971A040_c_10.tif: 8192x8192 794 qanats, 758 qanat_pairs, 65074.3ms\n",
      "Speed: 48.2ms pre-process, 65074.3ms inference, 154.2ms NMS per image at shape (1, 3, 8192, 8192)\n",
      "Results saved to \u001b[1mruns\\detect\\exp10\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp10\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:\\UnderTheSand\\Rasters\\ML\\AFG1\\D3C1219-300971A040_c_10.tif --img 8192 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cc9ba-1dac-4529-be2b-3ee5487a6beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  D:\\UnderTheSand\\Rasters\\ML\\AFG1\\D3C1209-300484A022_a_10.tif: 5120x5120 (yolov)\t3102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383db9d4-b17f-437a-9f36-882030535e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['C:/ML/yolov9/runs/train/exp/weights/best.pt'], source=D:\\UnderTheSand\\Rasters\\ML\\AFG1\\D3C1209-300484A022_a_10.tif, data=data\\coco128.yaml, imgsz=[4096, 4096], conf_thres=0.1, iou_thres=0.45, max_det=9999999999, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v0.1-28-g8a17325 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "yolov9_qanat summary: 580 layers, 60498220 parameters, 0 gradients, 263.9 GFLOPs\n",
      "image 1/1 D:\\UnderTheSand\\Rasters\\ML\\AFG1\\D3C1209-300484A022_a_10.tif: 4096x4096 701 qanats, 669 qanat_pairs, 1400.0ms\n",
      "Speed: 8.5ms pre-process, 1400.0ms inference, 147.9ms NMS per image at shape (1, 3, 4096, 4096)\n",
      "Results saved to \u001b[1mruns\\detect\\exp11\u001b[0m\n",
      "1 labels saved to runs\\detect\\exp11\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights C:/ML/yolov9/runs/train/exp/weights/best.pt --conf 0.1 --source D:\\UnderTheSand\\Rasters\\ML\\AFG1\\D3C1209-300484A022_a_10.tif --img 4096 --save-txt --save-conf --max-det 9999999999 --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0774fce-1db8-4e4b-bd33-52e57369db90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59873c9c-d215-4b01-b0c7-a220d85f2e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Yollo v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a7a0d-fc52-48ce-968b-ee872c3e1562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
